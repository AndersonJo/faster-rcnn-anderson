import itertools
import os
from typing import List, Tuple

import cv2
import numpy as np

from frcnn.anchor import to_absolute_coord
from frcnn.config import singleton_config
from frcnn.tools import denormalize_image, cal_fen_output_size


class RPNTrainerDebug:
    @staticmethod
    def debug_next_batch(image, meta, cls, reg):
        config = singleton_config()
        scales = [scale * np.array(config.anchor_ratios) for scale in config.anchor_scales]
        scales = np.array(scales).reshape(9, 2)

        height, width, _ = image.shape
        image = denormalize_image(image)

        width_ratio = meta['rescaled_width'] / meta['width']
        height_ratio = meta['rescaled_height'] / meta['height']

        fen_w, fen_h, _ = cal_fen_output_size('vgg19', width, height)

        # Check Classification
        cls_h, cls_w, cls_o = np.where(cls[0, :, :, 9:] == 1)
        reg = reg[0].copy()

        import ipdb
        ipdb.set_trace()
        for i in range(len(cls_h)):
            loc_w = cls_w[i]
            loc_h = cls_h[i]
            loc_o = cls_o[i]

            cw = (loc_w + 0.5) #* config.anchor_stride[0]
            ch = (loc_h + 0.5) #* config.anchor_stride[1]
            w, h = scales[loc_o]

            tx, ty, tw, th = reg[loc_h, loc_w, (loc_o * 4) + 36:(loc_o * 4) + 4 + 36]
            min_x = cw - w / 2
            min_y = ch - h / 2
            max_x = cw + w / 2
            max_y = ch + w / 2

            min_x = int(min_x)
            min_y = int(min_y)
            max_x = int(max_x)
            max_y = int(max_y)

            g_cx, g_cy, g_w, g_h = to_absolute_coord([min_x, min_y, max_x, max_y], [tx, ty, tw, th])
            g_x1 = int(g_cx - g_w / 2)
            g_y1 = int(g_cy - g_h / 2)
            g_x2 = int(g_cx + g_w / 2)
            g_y2 = int(g_cy + g_h / 2)

            # print(min_x, min_y, max_x, max_y, 'sxxxxxxxx', g_x1, g_y1, g_x2, g_y2)
            cv2.rectangle(image, (g_x1, g_y1), (g_x1 + 5, g_y1 + 5), (255, 255, 0))
            cv2.rectangle(image, (g_x2, g_y2), (g_x2 + 5, g_y2 + 5), (0, 255, 255))

        # Ground Truth
        for name, x1, y1, x2, y2 in meta['objects']:
            x1 *= width_ratio
            y1 *= height_ratio
            x2 *= width_ratio
            y2 *= height_ratio

            x1 = int(x1)
            y1 = int(y1)
            x2 = int(x2)
            y2 = int(y2)
            cv2.rectangle(image, (x1, y1), (x2, y2), (0, 0, 255))

        cv2.imwrite('temp/' + meta['filename'], image)


class TestRPN:
    """
    The class is used in `rpn_trainer.py -> RPNTargetProcessor -> generate_rpn_target`
    """

    @staticmethod
    def rectangle(image, x_pos: int, y_pos: int, anc_scale: int, anc_rat: List[float]):
        w, h = anc_scale * anc_rat[0], anc_scale * anc_rat[1]
        cv2.rectangle(image, (int(x_pos * 16 - w / 2), int(y_pos * 16 - h / 2)),
                      (int(x_pos * 16 + w / 2), int(y_pos * 16 + h / 2)),
                      (0, 0, 255))

    @staticmethod
    def point(image, x_pos: int, y_pos: int, color=(0, 0, 255)):
        cv2.rectangle(image, (x_pos * 16, y_pos * 16), (x_pos * 16 + 5, y_pos * 16 + 5), color)

    @staticmethod
    def apply(_image, x_pos, y_pos, anc_scale: int, anc_rat: List[float], reg_target):
        config = singleton_config()
        w, h = anc_scale * anc_rat[0], anc_scale * anc_rat[1]
        min_x = int(x_pos * config.anchor_stride[0] - w / 2)
        min_y = int(y_pos * config.anchor_stride[1] - h / 2)
        max_x = int(min_x + w)
        max_y = int(min_y + h)

        g_cx, g_cy, g_w, g_h = to_absolute_coord([min_x, min_y, max_x, max_y], reg_target)
        g_x1 = int(g_cx - g_w / 2)
        g_y1 = int(g_cy - g_h / 2)
        g_x2 = int(g_cx + g_w / 2)
        g_y2 = int(g_cy + g_h / 2)

        cv2.rectangle(_image, (g_x1, g_y1), (g_x2, g_y2), (0, 0, 255))


class FRCNNDebug:
    @staticmethod
    def debug_generate_anchors(anchors: np.ndarray, probs, rescaled_image: np.ndarray, meta: dict):
        """
        Anchors는 청생 포인트로 이미지에 점을 찍고, Ground-truth anchor는 빨간색 박스로 표시를 한다.
            - 빨간색 박스: meta에서 이미지에 대한 박스위치가 잘 잡혔는지 확인
            - 청색 포인트: anchors가 빨간색 박스 근처에서 잡혔는지 확인
        :param anchors:
        :param rescaled_image:
        :param meta:
        :return:
        """
        config = singleton_config()

        # image = cv2.imread(meta['image_path'])
        # image = cv2.resize(image, (meta['rescaled_width'], meta['rescaled_height']))
        rescaled_image = rescaled_image.copy()
        rescaled_image = denormalize_image(rescaled_image).copy()
        ratio = meta['rescaled_ratio']

        for anchor, prob in zip(anchors, probs):
            min_x = anchor[0] * config.anchor_stride[0]
            min_y = anchor[1] * config.anchor_stride[1]
            max_x = anchor[2] * config.anchor_stride[0] + min_x
            max_y = anchor[3] * config.anchor_stride[1] + min_y
            cx = (min_x + max_x) // 2
            cy = (min_y + max_y) // 2
            cv2.rectangle(rescaled_image, (cx, cy), (cx + 5, cy + 5), (255 * prob, 255 * prob, 0))

        for obj in meta['objects']:
            min_x, min_y, max_x, max_y = obj[1:]
            min_x = int(min_x * ratio)
            min_y = int(min_y * ratio)
            max_x = int(max_x * ratio)
            max_y = int(max_y * ratio)

            cx = (min_x + max_x) // 2
            cy = (min_y + max_y) // 2
            cv2.rectangle(rescaled_image, (min_x, min_y), (max_x, max_y), (0, 0, 255), thickness=1)

        cv2.imwrite(os.path.join('temp', meta['filename']), rescaled_image)


class ClassifierDebug:

    @classmethod
    def debug_images(cls, rois, loc_obj, loc_bg, img_meta, image):

        image = denormalize_image(image.copy()).copy()

        ratio_x = img_meta['rescaled_width'] / img_meta['width']
        ratio_y = img_meta['rescaled_height'] / img_meta['height']

        for roi in rois[0, loc_obj]:
            cls._rectangle(image, roi, color=(255, 255, 0), thickness=2)

        for roi in rois[0, loc_bg]:
            cls._point(image, roi, color=(200, 200, 200), thickness=2)

        for obj in img_meta['objects']:
            min_x, min_y, max_x, max_y = obj[1:]
            min_x = int(min_x * ratio_x)
            max_x = int(max_x * ratio_x)
            min_y = int(min_y * ratio_y)
            max_y = int(max_y * ratio_y)

            cx = (min_x + max_x) // 2
            cy = (min_y + max_y) // 2
            cv2.rectangle(image, (min_x, min_y), (max_x, max_y), (0, 0, 255), thickness=1)

        cv2.imwrite(os.path.join('temp', img_meta['filename']), image)

    @classmethod
    def _rectangle(cls, image, roi, color: Tuple[int, int, int] = (0, 0, 255), thickness=1):
        min_x = roi[0] * 16
        min_y = roi[1] * 16
        max_x = roi[2] * 16 + min_x
        max_y = roi[3] * 16 + min_y

        cx = (min_x + max_x) // 2
        cy = (min_y + max_y) // 2
        cv2.rectangle(image, (min_x, min_y), (max_x, max_y), color, thickness=thickness)

    @classmethod
    def _point(cls, image, roi, color: Tuple[int, int, int] = (0, 0, 255), thickness=1):
        min_x = roi[0] * 16
        min_y = roi[1] * 16
        max_x = roi[2] * 16 + min_x
        max_y = roi[3] * 16 + min_y

        cx = (min_x + max_x) // 2
        cy = (min_y + max_y) // 2
        cv2.rectangle(image, (cx - 5, cy - 5), (cx + 5, cy + 5), color, thickness=thickness)


def check_clf_trainer_classification(cls_y, meta, inv_class_mapping):
    _answer_class = [obj[0] for obj in meta['objects']]
    if len(set(_answer_class)) >= 2:
        _pred_class = [inv_class_mapping[idx] for idx in np.argmax(cls_y, axis=2).tolist()[0]]
        _pred_class = list(filter(lambda x: x != 'bg', _pred_class))

        print()
        print('predict:', _pred_class)
        print('answer:', _answer_class)
        print()
